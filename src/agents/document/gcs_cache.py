"""GCS cache utilities for checking and reading cached generated content.

This module provides functions to check if generated content (summary, FAQs, questions)
already exists in GCS and read it back, avoiding redundant LLM calls.
"""

import json
import logging
from dataclasses import dataclass
from typing import Optional, List, Dict, Any

from .tools import _build_content_path

logger = logging.getLogger(__name__)


@dataclass
class CachedSummary:
    """Cached summary content."""
    content: str
    word_count: int
    cached: bool = True


@dataclass
class CachedFAQ:
    """Single FAQ item."""
    question: str
    answer: str


@dataclass
class CachedFAQs:
    """Cached FAQs content."""
    faqs: List[CachedFAQ]
    count: int
    cached: bool = True


@dataclass
class CachedQuestion:
    """Single question item."""
    question: str
    expected_answer: str
    difficulty: str


@dataclass
class CachedQuestions:
    """Cached questions content."""
    questions: List[CachedQuestion]
    count: int
    difficulty_distribution: Dict[str, int]
    cached: bool = True


def _parse_summary_markdown(content: str) -> str:
    """
    Extract summary text from formatted markdown.

    Format expected:
    # Summary

    [Actual summary content here]

    ---
    _Generated by ...
    """
    # Find content between "# Summary\n\n" and "\n\n---"
    start_marker = "# Summary\n\n"
    end_marker = "\n\n---"

    start = content.find(start_marker)
    if start == -1:
        # No header found, return content as-is (minus any metadata footer)
        end = content.find("\n---\n")
        if end != -1:
            return content[:end].strip()
        return content.strip()

    # Extract content after the header
    content_start = start + len(start_marker)
    end = content.find(end_marker, content_start)

    if end != -1:
        return content[content_start:end].strip()

    # No end marker, return everything after header
    return content[content_start:].strip()


def _parse_faqs_json(content: str) -> List[Dict[str, str]]:
    """
    Extract FAQs list from JSON format.

    Format expected:
    {
        "metadata": {...},
        "faqs": [{"question": "...", "answer": "..."}, ...]
    }
    """
    try:
        data = json.loads(content)
        return data.get('faqs', [])
    except json.JSONDecodeError as e:
        logger.warning(f"Failed to parse FAQs JSON: {e}")
        return []


def _parse_questions_json(content: str) -> List[Dict[str, str]]:
    """
    Extract questions list from JSON format.

    Format expected:
    {
        "metadata": {...},
        "questions": [{"question": "...", "expected_answer": "...", "difficulty": "..."}, ...]
    }
    """
    try:
        data = json.loads(content)
        return data.get('questions', [])
    except json.JSONDecodeError as e:
        logger.warning(f"Failed to parse questions JSON: {e}")
        return []


async def check_and_read_cached_summary(
    parsed_file_path: str,
    document_name: str
) -> Optional[CachedSummary]:
    """
    Check if a summary exists in GCS and return it.

    Args:
        parsed_file_path: GCS path to parsed document (e.g., 'Acme corp/parsed/invoices/Sample1.md')
        document_name: Document filename (e.g., 'Sample1.md')

    Returns:
        CachedSummary if found, None otherwise
    """
    from src.storage.config import get_storage

    try:
        storage = get_storage()
        gcs_path = _build_content_path(parsed_file_path, 'summary', document_name)

        logger.debug(f"Checking GCS cache for summary: {gcs_path}")

        if not await storage.exists(gcs_path, use_prefix=False):
            logger.debug(f"Summary not found in GCS: {gcs_path}")
            return None

        content = await storage.read(gcs_path, use_prefix=False)
        if not content:
            logger.debug(f"Summary file empty: {gcs_path}")
            return None

        summary_text = _parse_summary_markdown(content)
        word_count = len(summary_text.split())

        logger.info(f"GCS cache hit for summary: {gcs_path} ({word_count} words)")

        return CachedSummary(
            content=summary_text,
            word_count=word_count,
            cached=True
        )

    except Exception as e:
        logger.warning(f"Error checking GCS cache for summary: {e}")
        return None


async def check_and_read_cached_faqs(
    parsed_file_path: str,
    document_name: str
) -> Optional[CachedFAQs]:
    """
    Check if FAQs exist in GCS and return them.

    Args:
        parsed_file_path: GCS path to parsed document
        document_name: Document filename

    Returns:
        CachedFAQs if found, None otherwise
    """
    from src.storage.config import get_storage

    try:
        storage = get_storage()
        gcs_path = _build_content_path(parsed_file_path, 'faqs', document_name)

        logger.debug(f"Checking GCS cache for FAQs: {gcs_path}")

        if not await storage.exists(gcs_path, use_prefix=False):
            logger.debug(f"FAQs not found in GCS: {gcs_path}")
            return None

        content = await storage.read(gcs_path, use_prefix=False)
        if not content:
            logger.debug(f"FAQs file empty: {gcs_path}")
            return None

        faqs_list = _parse_faqs_json(content)
        if not faqs_list:
            logger.debug(f"No FAQs parsed from: {gcs_path}")
            return None

        faqs = [
            CachedFAQ(question=f.get('question', ''), answer=f.get('answer', ''))
            for f in faqs_list
            if f.get('question') and f.get('answer')
        ]

        logger.info(f"GCS cache hit for FAQs: {gcs_path} ({len(faqs)} FAQs)")

        return CachedFAQs(
            faqs=faqs,
            count=len(faqs),
            cached=True
        )

    except Exception as e:
        logger.warning(f"Error checking GCS cache for FAQs: {e}")
        return None


async def check_and_read_cached_questions(
    parsed_file_path: str,
    document_name: str
) -> Optional[CachedQuestions]:
    """
    Check if questions exist in GCS and return them.

    Args:
        parsed_file_path: GCS path to parsed document
        document_name: Document filename

    Returns:
        CachedQuestions if found, None otherwise
    """
    from src.storage.config import get_storage

    try:
        storage = get_storage()
        gcs_path = _build_content_path(parsed_file_path, 'questions', document_name)

        logger.debug(f"Checking GCS cache for questions: {gcs_path}")

        if not await storage.exists(gcs_path, use_prefix=False):
            logger.debug(f"Questions not found in GCS: {gcs_path}")
            return None

        content = await storage.read(gcs_path, use_prefix=False)
        if not content:
            logger.debug(f"Questions file empty: {gcs_path}")
            return None

        questions_list = _parse_questions_json(content)
        if not questions_list:
            logger.debug(f"No questions parsed from: {gcs_path}")
            return None

        questions = [
            CachedQuestion(
                question=q.get('question', ''),
                expected_answer=q.get('expected_answer', ''),
                difficulty=q.get('difficulty', 'medium')
            )
            for q in questions_list
            if q.get('question')
        ]

        # Calculate difficulty distribution
        distribution = {"easy": 0, "medium": 0, "hard": 0}
        for q in questions:
            if q.difficulty.lower() in distribution:
                distribution[q.difficulty.lower()] += 1

        logger.info(f"GCS cache hit for questions: {gcs_path} ({len(questions)} questions)")

        return CachedQuestions(
            questions=questions,
            count=len(questions),
            difficulty_distribution=distribution,
            cached=True
        )

    except Exception as e:
        logger.warning(f"Error checking GCS cache for questions: {e}")
        return None
