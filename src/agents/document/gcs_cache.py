"""GCS cache utilities for checking and reading cached generated content.

This module provides functions to check if generated content (summary, FAQs, questions)
already exists in GCS and read it back, avoiding redundant LLM calls.
"""

import json
import logging
from dataclasses import dataclass
from typing import Optional, List, Dict, Any

from .tools import _build_content_path

logger = logging.getLogger(__name__)


@dataclass
class CachedSummary:
    """Cached summary content."""
    content: str
    word_count: int
    cached: bool = True


@dataclass
class CachedFAQ:
    """Single FAQ item."""
    question: str
    answer: str


@dataclass
class CachedFAQs:
    """Cached FAQs content."""
    faqs: List[CachedFAQ]
    count: int
    cached: bool = True


@dataclass
class CachedQuestion:
    """Single question item."""
    question: str
    expected_answer: str
    difficulty: str


@dataclass
class CachedQuestions:
    """Cached questions content."""
    questions: List[CachedQuestion]
    count: int
    difficulty_distribution: Dict[str, int]
    cached: bool = True


def _parse_summary_markdown(content: str) -> str:
    """
    Extract summary text from formatted markdown.

    Format expected:
    # Summary

    [Actual summary content here]

    ---
    _Generated by ...
    """
    start_marker = "# Summary\n\n"
    # Use specific footer pattern to avoid matching "---" within summary content
    footer_marker = "\n---\n_Generated by"

    start = content.find(start_marker)
    if start == -1:
        # No header found, search for footer pattern
        end = content.find(footer_marker)
        if end != -1:
            return content[:end].strip()
        # Fallback: try just the horizontal rule at end
        end = content.rfind("\n---\n")
        if end != -1:
            return content[:end].strip()
        return content.strip()

    # Extract content after the header
    content_start = start + len(start_marker)
    end = content.find(footer_marker, content_start)

    if end != -1:
        return content[content_start:end].strip()

    # No footer marker found, try to find last "---" separator
    end = content.rfind("\n---\n", content_start)
    if end != -1:
        return content[content_start:end].strip()

    # No end marker at all, return everything after header
    return content[content_start:].strip()


def _parse_faqs_json(content: str) -> List[Dict[str, str]]:
    """
    Extract FAQs list from JSON format.

    Format expected:
    {
        "metadata": {...},
        "faqs": [{"question": "...", "answer": "..."}, ...]
    }

    Also handles malformed nested format from older persist logic:
    {
        "metadata": {...},
        "faqs": {"success": true, "faqs": [...]}
    }
    """
    try:
        data = json.loads(content)
        # Handle double-encoded JSON
        if isinstance(data, str):
            data = json.loads(data)

        faqs = data.get('faqs', [])

        # Handle nested structure from old persist bug
        if isinstance(faqs, dict) and 'faqs' in faqs:
            faqs = faqs['faqs']

        # Validate it's a list
        if not isinstance(faqs, list):
            logger.warning(f"FAQs is not a list: {type(faqs)}")
            return []

        return faqs
    except json.JSONDecodeError as e:
        logger.warning(f"Failed to parse FAQs JSON: {e}")
        return []


def _parse_questions_json(content: str) -> List[Dict[str, str]]:
    """
    Extract questions list from JSON format.

    Format expected:
    {
        "metadata": {...},
        "questions": [{"question": "...", "expected_answer": "...", "difficulty": "..."}, ...]
    }

    Also handles malformed nested format from older persist logic:
    {
        "metadata": {...},
        "questions": {"success": true, "questions": [...]}
    }
    """
    try:
        data = json.loads(content)
        # Handle double-encoded JSON
        if isinstance(data, str):
            data = json.loads(data)

        questions = data.get('questions', [])

        # Handle nested structure from old persist bug
        if isinstance(questions, dict) and 'questions' in questions:
            questions = questions['questions']

        # Validate it's a list
        if not isinstance(questions, list):
            logger.warning(f"Questions is not a list: {type(questions)}")
            return []

        return questions
    except json.JSONDecodeError as e:
        logger.warning(f"Failed to parse questions JSON: {e}")
        return []


async def check_and_read_cached_summary(
    parsed_file_path: str,
    document_name: str
) -> Optional[CachedSummary]:
    """
    Check if a summary exists in GCS and return it.

    Args:
        parsed_file_path: GCS path to parsed document (e.g., 'Acme corp/parsed/invoices/Sample1.md')
        document_name: Document filename (e.g., 'Sample1.md')

    Returns:
        CachedSummary if found, None otherwise
    """
    from src.storage.config import get_storage

    try:
        storage = get_storage()
        gcs_path = _build_content_path(parsed_file_path, 'summary', document_name)

        logger.debug(f"Checking GCS cache for summary: {gcs_path}")

        if not await storage.exists(gcs_path, use_prefix=False):
            logger.debug(f"Summary not found in GCS: {gcs_path}")
            return None

        content = await storage.read(gcs_path, use_prefix=False)
        if not content:
            logger.debug(f"Summary file empty: {gcs_path}")
            return None

        summary_text = _parse_summary_markdown(content)
        word_count = len(summary_text.split())

        logger.info(f"GCS cache hit for summary: {gcs_path} ({word_count} words)")

        return CachedSummary(
            content=summary_text,
            word_count=word_count,
            cached=True
        )

    except Exception as e:
        logger.warning(f"Error checking GCS cache for summary: {e}")
        return None


async def check_and_read_cached_faqs(
    parsed_file_path: str,
    document_name: str
) -> Optional[CachedFAQs]:
    """
    Check if FAQs exist in GCS and return them.

    Args:
        parsed_file_path: GCS path to parsed document
        document_name: Document filename

    Returns:
        CachedFAQs if found, None otherwise
    """
    from src.storage.config import get_storage

    try:
        storage = get_storage()
        gcs_path = _build_content_path(parsed_file_path, 'faq', document_name)

        logger.debug(f"Checking GCS cache for FAQs: {gcs_path}")

        if not await storage.exists(gcs_path, use_prefix=False):
            logger.debug(f"FAQs not found in GCS: {gcs_path}")
            return None

        content = await storage.read(gcs_path, use_prefix=False)
        if not content:
            logger.debug(f"FAQs file empty: {gcs_path}")
            return None

        faqs_list = _parse_faqs_json(content)
        if not faqs_list:
            logger.debug(f"No FAQs parsed from: {gcs_path}")
            return None

        faqs = [
            CachedFAQ(question=f.get('question', ''), answer=f.get('answer', ''))
            for f in faqs_list
            if f.get('question') and f.get('answer')
        ]

        logger.info(f"GCS cache hit for FAQs: {gcs_path} ({len(faqs)} FAQs)")

        return CachedFAQs(
            faqs=faqs,
            count=len(faqs),
            cached=True
        )

    except Exception as e:
        logger.warning(f"Error checking GCS cache for FAQs: {e}")
        return None


async def check_and_read_cached_questions(
    parsed_file_path: str,
    document_name: str
) -> Optional[CachedQuestions]:
    """
    Check if questions exist in GCS and return them.

    Args:
        parsed_file_path: GCS path to parsed document
        document_name: Document filename

    Returns:
        CachedQuestions if found, None otherwise
    """
    from src.storage.config import get_storage

    try:
        storage = get_storage()
        gcs_path = _build_content_path(parsed_file_path, 'questions', document_name)

        logger.debug(f"Checking GCS cache for questions: {gcs_path}")

        if not await storage.exists(gcs_path, use_prefix=False):
            logger.debug(f"Questions not found in GCS: {gcs_path}")
            return None

        content = await storage.read(gcs_path, use_prefix=False)
        if not content:
            logger.debug(f"Questions file empty: {gcs_path}")
            return None

        questions_list = _parse_questions_json(content)
        if not questions_list:
            logger.debug(f"No questions parsed from: {gcs_path}")
            return None

        questions = [
            CachedQuestion(
                question=q.get('question', ''),
                expected_answer=q.get('expected_answer', ''),
                difficulty=q.get('difficulty', 'medium')
            )
            for q in questions_list
            if q.get('question')
        ]

        # Calculate difficulty distribution
        distribution = {"easy": 0, "medium": 0, "hard": 0}
        for q in questions:
            if q.difficulty.lower() in distribution:
                distribution[q.difficulty.lower()] += 1

        logger.info(f"GCS cache hit for questions: {gcs_path} ({len(questions)} questions)")

        return CachedQuestions(
            questions=questions,
            count=len(questions),
            difficulty_distribution=distribution,
            cached=True
        )

    except Exception as e:
        logger.warning(f"Error checking GCS cache for questions: {e}")
        return None
